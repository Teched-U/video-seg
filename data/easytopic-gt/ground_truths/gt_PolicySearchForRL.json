{"0": "Reinforcement learning - policy optimization ", "39": "Reinforcement Learning ", "82": "Policy optimization ", "123": "Policy optimization - 1 ", "220": "Why policy optimization ", "309": "Example Policy Optimization Success Stories ", "445": "Policy Optimization in the RL Landscape ", "497": "Outline", "589": "Pathwise DerivaRves (PD) / BackPropagation Through Time (BPTT) ", "654": "Pathwise DerivaRves (PD) / BackPropagation Through Time (BPTT)  - 1", "836": "Path Derivative for Stochastic f - Additive Noise ", "882": "Path Derivative for Stochastic f - reparameterization trick", "924": "Stochastic Dynamics f", "941": "Stochastic f, R and \u21e1\u2713", "962": "Stochastic f, R and \u21e1\u2713 and s0 ", "983": "PD/BPTT Policy Gradients: Complete Algorithm ", "1256": "SVG(inf)", "1325": "SVG variants ", "1466": "SVG(1)", "1629": "SVG(0)", "1817": "SVG(k)", "1839": "SVG(0) -> DPG ", "1889": "Deep Deterministic Policy Gradient (DDPG)", "1953": "DDPG Results ", "2044": "Outline - 1", "2079": "Black Box Gradient Computation", "2102": "Solution 2: Fix random seed", "2147": "Solution 2: Fix random seed - 1", "2184": "Solution 2: Fix random seed - 2", "2209": "Learning to Hover ", "2245": "Gradient-Free Methods", "2279": "Cross-Entropy Method ", "2374": "Cross-Entropy Method - 1", "2414": "Closely Related Approaches", "2508": "Applications", "2562": "Cross-Entropy / Evolutionary Methods", "2594": "Considerations", "2751": "Outline - 2", "3159": "Likelihood Ratio Policy Gradient", "3230": "Likelihood Ratio Policy Gradient - 1", "3388": "Derivation from Importance Sampling", "3505": "Likelihood Ratio Gradient: Validity ", "3540": "Likelihood Ratio Gradient: Intuition", "3640": "Let\u2019s decompose path into states and actions ", "3711": "Likelihood ratio gradient estimate", "3731": "Likelihood ratio gradient estimate - 1", "3756": "Likelihood ratio gradient estimate: baseline", "3851": "Likelihood ratio and temporal structure", "3909": "Pseudo-code reinforce aka vanilla policy gradient", "3962": "Outline - 3", "3963": "Outline - 3", "3975": "Step-sizing and trust regions", "3990": "What\u2019s in a step-size? ", "4046": "Step-sizing and trust regions - 1", "4085": "Step-sizing and trust regions - 2", "4115": "Evaluating the KL", "4159": "Evaluating the KL - 1", "4264": "EvaluaRng the KL - 2", "4304": "EvaluaRng the KL - 3", "4383": "Experiments in LocomoRon ", "4434": "Learning Curves - Comparison", "4448": "Learning Curves - Comparison - 1", "4453": "Atari Games", "4489": "Outline - 4", "4500": "Recall Our Likelihood RaRo PG EsRmator", "4559": "Estimation of V\u21e1", "4607": "Recall Our Likelihood Ratio PG Estimator ", "4679": "Variance Reduction by Discounting", "4697": "Reducing Variance by Function Approximation ", "4740": "Reducing Variance by Function Approximation - 1", "4772": "Actor-Critic with A3C or GAE ", "4889": "Async Advantage Actor Critic (A3C)", "4914": "A3C - labyrinth ", "4944": "GAE: Effect of gamma and lambda ", "5001": "Learning LocomoRon (TRPO + GAE) ", "5103": "Outline - 5", "5122": "Stochastic Computation Graphs ", "5204": "Food for thought", "5248": "Current frontiers", "5263": "Current frontiers - 1", "5276": "How to learn more and get started? ", "5283": "How to learn more and get started?  - 1", "5288": "How to learn more and get started? - 2"}