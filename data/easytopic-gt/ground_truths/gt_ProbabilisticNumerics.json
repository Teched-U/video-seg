{"0": "Probabilistic numerics for deep learning", "127": "Probabilistic numerics treats computation as a decision - 1", "143": "Probabilistic numerics treats computation as a decision - 2", "150": "Probabilistic numerics treats computation as a decision - 3", "166": "Probabilistic numerics is the study of numeric methods as learning algorithms.", "210": "Global optimisation considers objective functions that are multi-modal and often expensive to evaluate. ", "263": "The Rosenbrock is expressible in closed-form.", "349": "Computational limits form the core of the optimisation problem.", "392": "We are epistemically uncertain about f(x,y) due to being unable to afford its computation.", "482": "Probabilistic modelling of functions", "501": "Probability theory represents an extension of traditional logic, allowing us to reason in the face of uncertainty.", "619": "A probability is a degree of belief. This might be held by any agent \u2013 a human, a robot, a pigeon, etc.", "640": "\u2018I\u2019 is the totality of an agent\u2019s prior information. An agent is (partially) defined by I.", "666": "A probability is a degree of belief. This might be held by any agent \u2013 a human, a robot, a pigeon, etc.", "701": "\u2018I\u2019 is the totality of an agent\u2019s prior information. An agent is (partially) defined by I.", "809": "The Gaussian distribution allows us to produce distributions for variables conditioned on any other observed variables", "1008": "A Gaussian process is the generalisation of a multivariate Gaussian distribution to a potentially infinite number of variables.", "1076": "A Gaussian process provides a non-parametric model for functions, defined by mean and covariance functions. ", "1098": "Gaussian processes are specified by a covariance function, which flexibly allow the expression of e.g", "1144": "Gaussian processes have a complexity that grows with the data; they provide flexible models, robust to overfitting.  - 1", "1209": "Gaussian processes have a complexity that grows with the data; they provide flexible models, robust to overfitting. - 2", "1222": "Gaussian processes have a complexity that grows with the data; they provide flexible models, robust to overfitting.  - 4", "1240": "Gaussian processes have a complexity that grows with the data; they provide flexible models, robust to overfitting.  - 5", "1241": "Bayesian optimisation as decision theory", "1259": "Bayesian\n optimisation is the approach of probabilistically modelling f(x,y), and\n using decision theory to make optimal use of computation", "1291": "By\n defining the costs of observation and uncertainty, we can select \nevaluations optimally by minimising the expected loss with respect to a \nprobability distribution.", "1339": "We define a loss function that is the lowest function value found after our algorithm ends.", "1470": "This\n loss function makes computing the expected loss simple: we\u2019ll take a \nmyopic approximation and consider only the next evaluation.", "1522": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 1", "1633": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 2", "1696": "untitled", "1730": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 4", "1744": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 5", "1753": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 6", "1760": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 7", "1766": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 8", "1985": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 9", "1986": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 10", "1991": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 9", "2199": "We choose a Gaussian process as the probability distribution for the objective function, giving a tractable expected loss. - 10", "2209": "Tuning is used to cope with model parameters (such as periods).", "2319": "Bayesian optimisation gives a powerful method for such tuning.", "2341": "Snoek, Larochelle and Adams (2012) used Bayesian optimisation to tune convolutional neural networks. ", "2509": "Bayesian\n optimisation is useful in automating structured search over # hidden \nlayers, learning rates, dropout rates, # hidden units per layer & L2\n weight constraints.", "2553": "Bayesian stochastic optimisation", "2568": "Using only a subset of the data (a mini-batch) gives a noisy likelihood evaluation", "2676": "If we use Bayesian optimisation on these noisy evaluations, we can perform stochastic learning.", "2723": "Lower-variance\n evaluations (on smaller subsets) are higher cost: let\u2019s also Bayesian \noptimise over the fidelity of our evaluations!", "2942": "Quiz: which of these sequences is random? - 1", "3020": "Quiz: which of these sequences is random? - 2", "3139": "A random number", "3379": "Integration beats optimisation", "3402": "The na\u00efve fitting of models to data performed by optimisation can lead to overfitting.", "3428": "Bayesian averaging over ensembles of models reduces overfitting, and provides more honest estimates of uncertainty", "3464": "Our model", "3584": "Averaging requires integrating over the many possible states of the world consistent with data: this is often non-analytic.", "3612": "Numerical integration (quadrature) is ubiquitous. ", "3713": "Optimisation is an unreasonable way of estimating a multi-modal or broad likelihood integrand.", "3807": "If optimising, flat optima are often a better representation of the integral than narrow optima. ", "3869": "Bayesian\n quadrature makes use of a Gaussian process surrogate for the integrand \n(the same as you might use for Bayesian optimisation).", "3932": "Gaussian distributed variables are joint Gaussian with any affine transform of them. ", "4019": "A\n function over which we have a Gaussian process is joint Gaussian with \nany integral or derivative of it, as integration and differentiation are\n linear.", "4069": "We\n can use observations of an integrand \u2113 in order to perform inference \nfor its integral, Z: this is known as Bayesian Quadrature.", "4162": "Bayesian quadrature generalises and improves upon traditional quadrature.", "4727": "Quiz: what is the convergence rate of Monte Carlo? - 1", "4776": "Quiz: what is the convergence rate of Monte Carlo? - 2", "4836": "Monte Carlo", "4925": "Probabilistic numerics views the selection of samples as a decision problem.", "5385": "Our method (Warped Sequential Active Bayesian Integration) converges quickly in wall-clock time for a synthetic integrand.", "5392": "WSABI-L converges quickly in integrating out hyperparameters in a Gaussian process classification problem (CiteSeerx data).", "5393": "Probabilistic numerics offers the propagation of uncertainty through numerical pipelines.", "5395": "Probabilistic numerics treats computation as a decision"}