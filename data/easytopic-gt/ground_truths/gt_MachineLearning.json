{"0": "Introduction to Machine Learning", "16": "Outline", "36": "Types of machine learning problems", "46": "Supervised learning", "122": "Example: Face detection and recognition", "143": "Reinforcement learning", "174": "Example: TD-Gammon (Tesauro, 1990-1995)", "244": "Unsupervised learning", "261": "Example: Oncology (Alizadeh et al.)", "306": "Example: A data set", "342": "Data (continued)", "382": "Terminology", "402": "More formally", "421": "Supervised learning problem", "469": "Steps to solving a supervised learning problem", "518": "Example: What hypothesis class should we pick?", "550": "Linear hypothesis", "591": "Error minimization!", "600": "Least mean squares (LMS)", "638": "Steps to solving a supervised learning problem", "641": "Notation reminder", "671": "A bit of algebra", "698": "The solution", "740": "Example: Data and best linear hypothesis", "743": "Linear regression summary", "747": "Linear function approximation in general", "819": "Linear models in general", "863": "Remarks", "955": "Order-2 fit", "979": "Order-3 fit", "983": "Order-4 fit", "985": "Order-5 fit", "987": "Order-6 fit", "989": "Order-7 fit", "995": "Order-8 fit", "999": "Order-9 fit", "1038": "Order-8 fit", "1062": "Order-2 fit", "1096": "Overfitting", "1103": "Overfitting and underfitting", "1240": "Overfitting more formally", "1320": "Typical overfitting plot", "1431": "Cross-validation", "1492": "The anatomy of the error of an estimator", "1570": "Bias-variance analysis", "1634": "Recall: Statistics 101", "1654": "Bias-variance decomposition", "1688": "Bias-variance decomposition (2)", "1833": "Error decomposition", "1924": "Bias-variance decomposition (2)", "1947": "Bias-variance decomposition", "2019": "Bias-variance trade-off", "2057": "More on overfitting", "2122": "Coming back to mean-squared error function...", "2155": "A probabilistic assumption", "2196": "Bayes theorem in learning", "2279": "Choosing hypotheses", "2342": "Maximum likelihood estimation", "2373": "The log trick", "2425": "Maximum likelihood for regression", "2430": "The log trick", "2448": "Maximum likelihood for regression", "2467": "Applying the log trick", "2663": "Maximum likelihood hypothesis for least-squares estimators", "2672": "A graphical representation for the data generation", "2822": "Regularization", "2880": "Regularization for linear models", "3046": "What L2 regularization does", "3063": "Visualizing regularization (2 parameters)", "3182": "Pros and cons of L2 regularization", "3233": "L1 Regularization for linear models", "3297": "Solving L1 regularization", "3328": "Visualizing L1 regularization", "3330": "Pros and cons of L1 regularization", "3367": "Example of L1 vs L2 effect", "3410": "Bayesian view of regularization - 1", "3545": "Bayesian view of regularization - 2", "3757": "What does the Bayesian view give us? - 1", "3827": "What does the Bayesian view give us? - 2", "3885": "What does the Bayesian view give us? - 3", "4407": "Logistic regression", "4554": "The cross-entropy error function", "4643": "Cross-entropy error surface for logistic function", "4684": "Gradient descent", "4727": "Example gradient descent traces", "4754": "Gradient descent algorithm", "4818": "Maximization procedure: Gradient ascent", "4876": "Another algorithm for optimization", "4911": "Application to machine learning", "4933": "Second-order methods: Multivariate setting", "4992": "Which method is better?", "5027": "Newton-Raphson for logistic regression", "5049": "Regularization for logistic regression", "5110": "Probabilistic view of logistic regression", "5117": "Recap"}