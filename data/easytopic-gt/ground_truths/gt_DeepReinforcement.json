{"0": "Deep reinforcement learning", "28": "History", "103": "Artificial intelligence", "227": "Reinforcement learning", "329": "Reinforcement learning - 1", "480": "Reinforcement learning - 2", "540": "Approaches to reinforcement learning", "693": "Reinforcement learning - 3", "837": "Deep reinforcement learning", "954": "Deep Q Networks", "975": "Q-learning", "1370": "Q-learning - 1", "1414": "DQN", "1694": "DQN - 1", "1914": "DQN - 2", "2246": "Target Network Intuition", "2639": "Experience replay", "2692": "DQN - 3", "2703": "Experience replay - 1", "2785": "Prioritized Experience Replay", "3304": "Double DQN", "3438": "Double DQN - 1", "3468": "Double DQN - 2", "3507": "Double DQN - 3", "3518": "Double DQN - 4", "3948": "Insights", "4036": "Dueling DQN", "4502": "Policy Gradient", "4536": "Policy Gradient Theorem", "4537": "Policy Gradient Theorem", "4579": "Contextual Bandit Policy Gradient", "4586": "Policy Gradient Theorem", "4623": "Variance Reduction", "4717": "AsyncRL", "4811": "Async Advantage Actor-Critic (A3C)"}